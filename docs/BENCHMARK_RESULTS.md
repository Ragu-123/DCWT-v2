# Benchmark: Legacy Field LLM vs Standard Transformer

> Note: This document is historical (pre-DCWT-v2).  
> Current benchmark entrypoints for the updated proposal are in `benchmarks/benchmark_wikitext2.py`, `benchmarks/wikitext2.py`, `benchmarks/train_wave_v35_bpe.py`, and `benchmarks/train_100m_bpe.py`.

**Date**: February 17, 2026  
**Dataset**: Full Shakespeare (1.1M chars, 32,777 lines)  
**Hardware**: NVIDIA A10G GPU (g5.4xlarge)  
**Setup**: Same tokenizer, same data, same hyperparameters, same epochs

---

## Model Configurations

| | Field LLM | Standard Transformer |
|---|---|---|
| Architecture | Causal Field Attention (FFT convolution) | PyTorch TransformerEncoder (self-attention) |
| Attention Complexity | **O(n)** | **O(n^2)** |
| Parameters | 5,116,480 | 5,034,240 |
| Embedding Dim | 256 | 256 |
| Layers | 6 | 6 |
| Heads | 8 | 8 |
| FFN Dim | 1024 | 1024 |
| Dropout | 0.1 | 0.1 |

Both models use identical: tokenizer (FieldTokenizerV2, 1024 vocab), optimizer (AdamW, lr=0.0003), scheduler (warmup 5 epochs + cosine decay), batch size (32), epochs (50), gradient clipping (0.5), mixed precision (AMP).

---

## Training Results

| Metric | Field LLM | Standard Transformer | Gap |
|--------|-----------|---------------------|-----|
| **Best Val Loss** | 3.358 | 2.999 | +12.0% |
| **Best Val Perplexity** | 28.7 | 20.1 | +42.8% |
| **Best Val Accuracy** | 22.0% | 32.4% | -10.4pp |
| **Best Epoch** | 45 | 15 | -- |
| **Time per Epoch** | 33s | 16s | 2.1x slower |
| **Total Training Time** | ~28 min | ~13 min | 2.1x slower |
| **Final Train Loss** | 3.12 | 1.64 | -- |
| **Overfitting?** | No (val loss kept improving) | Yes (val loss rose after epoch 15) | -- |

---

## Training Curves

### Field LLM
```
Epoch  1: Val PPL 83.3, Acc  7.9%
Epoch  5: Val PPL 45.4, Acc 16.0%
Epoch 10: Val PPL 33.6, Acc 18.6%
Epoch 15: Val PPL 30.9, Acc 19.6%
Epoch 20: Val PPL 30.5, Acc 20.2%
Epoch 25: Val PPL 29.3, Acc 21.2%
Epoch 30: Val PPL 28.9, Acc 21.3%
Epoch 35: Val PPL 28.8, Acc 21.6%
Epoch 40: Val PPL 28.8, Acc 21.7%
Epoch 45: Val PPL 28.7, Acc 22.0%  ← BEST
Epoch 50: Val PPL 28.8, Acc 22.0%
```

### Standard Transformer
```
Epoch  1: Val PPL 50.7, Acc 16.5%
Epoch  5: Val PPL 25.5, Acc 23.9%
Epoch 10: Val PPL 21.0, Acc 29.1%
Epoch 15: Val PPL 20.1, Acc 31.5%  ← BEST (val loss)
Epoch 20: Val PPL 21.0, Acc 32.0%
Epoch 25: Val PPL 22.5, Acc 32.4%  ← BEST (accuracy)
Epoch 30: Val PPL 24.9, Acc 32.4%
Epoch 35: Val PPL 27.5, Acc 32.3%  (overfitting)
Epoch 40: Val PPL 29.6, Acc 32.3%  (overfitting)
Epoch 45: Val PPL 30.8, Acc 32.3%  (overfitting)
Epoch 50: Val PPL 31.6, Acc 32.3%  (overfitting)
```

---

## Generation Comparison (Temperature 0.7)

### "First Citizen:"
- **Field LLM**: *first citizen: commend me to my trealing. thou art too? is,---' tis gone! what's away*
- **Standard**: *first citizen: i had forged to give him thanks,-- but 'tis alw.' you green: sir, i pray*

### "To be or not to be"
- **Field LLM**: *to be or not to be anfeth, as they are done iroud.' ll go: if it not?*
- **Standard**: *to be or not to be so belovi' the war. go with us, let it: but if he be amends*

### "We are accounted poor"
- **Field LLM**: *we are accounted poor king, and ighers. 'tis gone: come?--- for,- of that shall r*
- **Standard**: *we are accounted poor citizens, our general has sworn.'- you have, sir? pity of the feck*

### "The quality of mercy is"
- **Field LLM**: *the quality of mercy is resmond, for you, who, i pray.'t:- and he were!*
- **Standard**: *the quality of mercy is no less, but it is not so. go you; come: let's to me.*

---

## Analysis

### Where Standard Transformer Wins
1. **Quality**: 42.8% better perplexity, 10pp better accuracy
2. **Speed at short sequences**: 2x faster per epoch at 128 tokens
3. **Grammar**: Produces more coherent, grammatical sentences
4. **Vocabulary**: Fewer nonsense words in generation

### Where Field LLM Wins
1. **No overfitting**: Val loss kept improving through epoch 45; standard overfits after epoch 15
2. **Stability**: Smooth, monotonic improvement vs standard's divergence
3. **Theoretical scaling**: O(n) vs O(n^2) -- advantage grows with sequence length
4. **Memory scaling**: O(G*D) vs O(n^2) -- can handle much longer sequences

### Key Insight: Overfitting

The standard transformer's val perplexity **went from 20.1 to 31.6** between epoch 15-50 (overfitting). By epoch 50, Field LLM (28.7) actually had **lower perplexity** than the overfitting standard transformer (31.6). This suggests Field LLM's field attention acts as a natural regularizer.

---

## Verdict

**Quality gap: ~43% on perplexity at short sequences (128 tokens).**

This falls in the "significant gap" range. At short sequences, the standard transformer wins clearly on quality.

**However**, this benchmark only tests at 128 tokens -- where Field LLM has zero structural advantage. The real test would be at 4K-128K tokens, where:
- Standard transformer memory grows as O(n^2) and may crash
- Field LLM memory stays flat at O(G*D)
- Field LLM's regularization effect may become more valuable

---

## What This Means for the Project

### Honest Assessment
Field LLM at 5M parameters on short sequences is **not competitive** with a standard transformer on quality. This is expected -- O(n^2) attention is more expressive than O(n) convolution.

### Path Forward
1. **Fix the tokenizer** -- 86.8% coverage is hurting quality. BPE could close some of the gap.
2. **Increase field_size** -- Reduce information loss from discretization
3. **Scale to 50-100M params** -- Quality gap may shrink at larger scale
4. **Benchmark at long sequences** -- This is where Field LLM's real advantage lives
5. **Hybrid approach** -- Use field attention for long-range + standard attention for local context

### The Real Question
Can Field LLM close the quality gap enough that the 100-500x cost savings at long sequences makes it worthwhile? The tokenizer and field size improvements could potentially close 10-20% of the gap. Scaling to 50M+ params could close more. If we can get within 15-20% of standard transformer quality, the architecture is commercially viable for long-context tasks.
